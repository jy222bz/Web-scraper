# A01 Web scraper
## Installation and Configurations:
1. ### Clone the repo:

~~~
clone git@gitlab.lnu.se:1dv523/student/jy222bz/a01-web-scraper.git
~~~

2. ### Install the dependencies:
~~~
npm install
~~~

3. ### Run the application:
~~~
npm start 'URL'
~~~
4. ### To run the eslint, run one of the following commands:

~~~
npm test
~~~
~~~
npm run lint
~~~
<br><br><br>
_____

# Report

1. The code was structured on modules and functions so that the modules can be reused, testable, and trackable. From a personal perspective, it is effective to structure the code in an object-oriented paradigm to allow flexibility, efficiency, reusability and quality. The architecture is basically modularized, and it is a Node.js application that is following the JavaScript Standard Style.

2. Node.js is Single Threaded, but it provides asynchronous and synchronous programming. Asynchronous programming allows code to be a non-blocking code where the next code can be executed before the previous one has done its execution. Synchronous programming allows a block of codes to execute in sequential before moving to the next block. New wanna-be-Node-programmers should learn Node because it is straightforward to learn and use, and it is beneficial for full-stack web development. It is straightforward to use with JavaScript. 

3. I am satisfied with the solution since it is modularized and uses different libraries, where I benefited from using them and learning when and where to optimize them. The solution has complexity and it is general to a great extent. Furthermore, the solution follows the best practices, and that is fundamental in any software development. Definitely, there is always room for improvements. One possible improvement the asynchronous programming can be improved further.

4. TIL: I have learned about Node, asynchronous and synchronous programming and learned how to use different libraries such as "Request-Promise", and "cheerio" to scrap information for analysis. Furthermore, I have learned about how to send RESTful requests with authentication and cockies. Moreover, I learned about how to access and manipulate the headers of the RESTful requests and analyze the data by using Postman. Additionally, I have learned about the importace of Node.js and how it works hand-in-hand with JavaScript.

